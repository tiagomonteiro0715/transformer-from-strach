# transformer-from-strach
Multi-head attention transformer from "Attention is all you need" paper code implementation
